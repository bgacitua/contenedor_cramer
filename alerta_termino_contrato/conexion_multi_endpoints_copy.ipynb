{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66cffa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "import requests\n",
    "#import mysql.connector\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "#import json\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3005fe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN: Xegy8dVsa1H8SFfojJcwYtDL\n",
      "SQL: _Cramer2025_\n",
      "IP: 192.168.245.33\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n API\n",
    "API_BASE_URL = \"https://cramer.buk.cl/api/v1/chile/\"\n",
    "API_ENDPOINTS = {\n",
    "    \"licences\": f\"{API_BASE_URL}absences/licence\",\n",
    "    \"absences\": f\"{API_BASE_URL}absences/absence\",\n",
    "    \"permissions\": f\"{API_BASE_URL}absences/permission\"\n",
    "}\n",
    "TOKEN = os.getenv(\"BUK_AUTH_TOKEN\")\n",
    "\n",
    "# Configuraci√≥n BD\n",
    "DB_HOST = os.getenv(\"IP\") #REEMPLAZAR HOST SI ES DISTINTO\n",
    "DB_USER = \"rrhh_master\" #REEMPLAZAR USUARIO CREADO POR GABRIEL Y CONTRASE√ëA\n",
    "DB_PASSWORD = os.getenv(\"clave_sql\") #REEMPLAZAR CONTRASE√ëA CREADA POR GABRIEL\n",
    "DB_NAME = \"rrhh_app\" #REEMPLAZAR NOMBRE DE BASE DE DATOS CREADA POR GABRIEL\n",
    "\n",
    "# Imprimir las variables de entorno\n",
    "print(f\"TOKEN: {os.getenv('BUK_AUTH_TOKEN')}\")\n",
    "print(f\"SQL: {os.getenv('clave_sql')}\")\n",
    "print(f\"IP: {os.getenv('IP')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a4aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóìÔ∏è FILTRO POR FECHAS ACTIVADO: Se extraer√°n datos entre 2025-09-05 y 2025-09-12.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuraci√≥n de Filtro por Rango de Fechas ---\n",
    "\n",
    "fecha_hoy = datetime.now().date()\n",
    "fecha_inicio_objetivo = fecha_hoy - timedelta(days=7)\n",
    "\n",
    "# Cambia a True para activar el filtro por fechas. Si es False, extraer√° todos los datos.\n",
    "FILTRAR_POR_FECHAS = True\n",
    "\n",
    "# Define el rango de fechas si FILTRAR_POR_FECHAS es True. Formato: \"YYYY-MM-DD\"\n",
    "# La API de BUK filtra por la fecha de inicio de la incidencia.\n",
    "FECHA_INICIO = fecha_inicio_objetivo.strftime(\"%Y-%m-%d\")\n",
    "FECHA_FIN = fecha_hoy.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "if FILTRAR_POR_FECHAS:\n",
    "    print(f\"üóìÔ∏è FILTRO POR FECHAS ACTIVADO: Se extraer√°n datos entre {FECHA_INICIO} y {FECHA_FIN}.\")\n",
    "else:\n",
    "    print(\"‚öôÔ∏è Extrayendo todos los datos disponibles (sin filtro de fechas).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f90eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Comenzando la obtenci√≥n de datos desde: https://cramer.buk.cl/api/v1/chile/absences/licence\n",
      "üìÑ Obteniendo p√°gina 1...\n",
      "‚ùå Error en la petici√≥n para la p√°gina 1: HTTPSConnectionPool(host='cramer.buk.cl', port=443): Max retries exceeded with url: /api/v1/chile/absences/licence (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000022768E50590>: Failed to resolve 'cramer.buk.cl' ([Errno 11001] getaddrinfo failed)\"))\n",
      "üéâ ¬°Paginaci√≥n completada! Total de datos obtenidos: 0\n",
      "--- Extracci√≥n de 'licences' terminada. ---\n",
      "Primer dato de 'licences': No data\n",
      "\n",
      "üöÄ Comenzando la obtenci√≥n de datos desde: https://cramer.buk.cl/api/v1/chile/absences/absence\n",
      "üìÑ Obteniendo p√°gina 1...\n",
      "‚ùå Error en la petici√≥n para la p√°gina 1: HTTPSConnectionPool(host='cramer.buk.cl', port=443): Max retries exceeded with url: /api/v1/chile/absences/absence (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000022768E54A50>: Failed to resolve 'cramer.buk.cl' ([Errno 11001] getaddrinfo failed)\"))\n",
      "üéâ ¬°Paginaci√≥n completada! Total de datos obtenidos: 0\n",
      "--- Extracci√≥n de 'absences' terminada. ---\n",
      "Primer dato de 'absences': No data\n",
      "\n",
      "üöÄ Comenzando la obtenci√≥n de datos desde: https://cramer.buk.cl/api/v1/chile/absences/permission\n",
      "üìÑ Obteniendo p√°gina 1...\n",
      "‚ùå Error en la petici√≥n para la p√°gina 1: HTTPSConnectionPool(host='cramer.buk.cl', port=443): Max retries exceeded with url: /api/v1/chile/absences/permission (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000022768E55450>: Failed to resolve 'cramer.buk.cl' ([Errno 11001] getaddrinfo failed)\"))\n",
      "üéâ ¬°Paginaci√≥n completada! Total de datos obtenidos: 0\n",
      "--- Extracci√≥n de 'permissions' terminada. ---\n",
      "Primer dato de 'permissions': No data\n"
     ]
    }
   ],
   "source": [
    "#Mostrar los datos del primer json de cada endpoint\n",
    "def mostrar_datos_endpoint(endpoint_url: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        list: Una lista con todos los datos obtenidos.\n",
    "    \"\"\"\n",
    "    headers = {\"auth_token\": TOKEN}\n",
    "    todos_los_datos = []\n",
    "    url_actual = endpoint_url\n",
    "    pagina_actual = 1\n",
    "\n",
    "    print(f\"\\nüöÄ Comenzando la obtenci√≥n de datos desde: {endpoint_url}\")\n",
    "\n",
    "    while url_actual and pagina_actual <= 1:\n",
    "        print(f\"üìÑ Obteniendo p√°gina {pagina_actual}...\")\n",
    "\n",
    "        try:\n",
    "            respuesta = requests.get(url_actual, headers=headers, timeout=10)\n",
    "            respuesta.raise_for_status()\n",
    "\n",
    "            respuesta_api = respuesta.json()\n",
    "            datos_pagina = respuesta_api.get('data', [])\n",
    "            pagination_info = respuesta_api.get('pagination', {})\n",
    "\n",
    "            todos_los_datos.extend(datos_pagina)\n",
    "\n",
    "            print(f\"‚úÖ P√°gina {pagina_actual}: {len(datos_pagina)} datos obtenidos\")\n",
    "            print(f\"üìä Total acumulado: {len(todos_los_datos)} datos\")\n",
    "            \n",
    "            total_pages = pagination_info.get('total_pages', 1)\n",
    "            print(f\"üìà P√°ginas restantes: {total_pages - pagina_actual}\")\n",
    "\n",
    "            url_actual = pagination_info.get('next')\n",
    "            pagina_actual += 1\n",
    "\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Error en la petici√≥n para la p√°gina {pagina_actual}: {e}\")\n",
    "            break\n",
    "\n",
    "    print(f\"üéâ ¬°Paginaci√≥n completada! Total de datos obtenidos: {len(todos_los_datos)}\")\n",
    "    return todos_los_datos\n",
    "\n",
    "# Itera sobre todos los endpoints y almacena los datos en un diccionario\n",
    "todos_los_datos_extraidos = {}\n",
    "\n",
    "for nombre_endpoint, url_endpoint in API_ENDPOINTS.items():\n",
    "    datos_obtenidos = mostrar_datos_endpoint(url_endpoint)\n",
    "    todos_los_datos_extraidos[nombre_endpoint] = datos_obtenidos\n",
    "    print(f\"--- Extracci√≥n de '{nombre_endpoint}' terminada. ---\")\n",
    "    print(f\"Primer dato de '{nombre_endpoint}': {datos_obtenidos[0] if datos_obtenidos else 'No data'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7754b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construir_url_con_fechas(base_url: str, fecha_inicio: str = None, fecha_fin: str = None):\n",
    "    \"\"\"\n",
    "    Construye la URL del endpoint con par√°metros de fecha si est√°n definidos.\n",
    "    \n",
    "    Args:\n",
    "        base_url (str): URL base del endpoint\n",
    "        fecha_inicio (str): Fecha de inicio en formato YYYY-MM-DD\n",
    "        fecha_fin (str): Fecha de fin en formato YYYY-MM-DD\n",
    "        \n",
    "    Returns:\n",
    "        str: URL completa con par√°metros de fecha si aplica\n",
    "    \"\"\"\n",
    "    if fecha_inicio and fecha_fin:\n",
    "        # Agregar par√°metros de fecha a la URL\n",
    "        separador = \"&\" if \"?\" in base_url else \"?\"\n",
    "        url_con_fechas = f\"{base_url}{separador}from={fecha_inicio}&to={fecha_fin}\"\n",
    "        return url_con_fechas\n",
    "    else:\n",
    "        return base_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb0f89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ --- Procesando endpoint: 'licences' ---\n",
      "\n",
      "üöÄ Comenzando extracci√≥n con filtro de fechas desde: https://cramer.buk.cl/api/v1/chile/absences/licence?from=2025-09-05&to=2025-09-12\n",
      "üìÑ Obteniendo p√°gina 1...\n",
      "‚úÖ P√°gina 1: 21 datos obtenidos\n",
      "üìä Total acumulado: 21 datos\n",
      "üìà P√°ginas restantes: 0\n",
      "üéâ ¬°Extracci√≥n con filtro de fechas completada! Total: 21 datos\n",
      "--- Extracci√≥n de 'licences' terminada. 21 registros obtenidos ---\n",
      "\n",
      "üîÑ --- Procesando endpoint: 'absences' ---\n",
      "\n",
      "üöÄ Comenzando extracci√≥n con filtro de fechas desde: https://cramer.buk.cl/api/v1/chile/absences/absence?from=2025-09-05&to=2025-09-12\n",
      "üìÑ Obteniendo p√°gina 1...\n",
      "‚úÖ P√°gina 1: 1 datos obtenidos\n",
      "üìä Total acumulado: 1 datos\n",
      "üìà P√°ginas restantes: 0\n",
      "üéâ ¬°Extracci√≥n con filtro de fechas completada! Total: 1 datos\n",
      "--- Extracci√≥n de 'absences' terminada. 1 registros obtenidos ---\n",
      "\n",
      "üîÑ --- Procesando endpoint: 'permissions' ---\n",
      "\n",
      "üöÄ Comenzando extracci√≥n con filtro de fechas desde: https://cramer.buk.cl/api/v1/chile/absences/permission?from=2025-09-05&to=2025-09-12\n",
      "üìÑ Obteniendo p√°gina 1...\n",
      "‚úÖ P√°gina 1: 2 datos obtenidos\n",
      "üìä Total acumulado: 2 datos\n",
      "üìà P√°ginas restantes: 0\n",
      "üéâ ¬°Extracci√≥n con filtro de fechas completada! Total: 2 datos\n",
      "--- Extracci√≥n de 'permissions' terminada. 2 registros obtenidos ---\n"
     ]
    }
   ],
   "source": [
    "def obtener_datos_paginados(endpoint_url: str, aplicar_filtro_fechas: bool = False, fecha_inicio: str = None, fecha_fin: str = None):\n",
    "    \"\"\"\n",
    "    Obtiene datos paginados desde un endpoint, con opci√≥n de filtrar por fechas.\n",
    "    \n",
    "    Args:\n",
    "        endpoint_url (str): URL del endpoint\n",
    "        aplicar_filtro_fechas (bool): Si aplicar filtro de fechas\n",
    "        fecha_inicio (str): Fecha de inicio en formato YYYY-MM-DD\n",
    "        fecha_fin (str): Fecha de fin en formato YYYY-MM-DD\n",
    "        \n",
    "    Returns:\n",
    "        list: Una lista con todos los datos obtenidos.\n",
    "    \"\"\"\n",
    "    headers = {\"auth_token\": TOKEN}\n",
    "    todos_los_datos = []\n",
    "    \n",
    "    # Construir la URL con filtros de fecha si es necesario\n",
    "    if aplicar_filtro_fechas and fecha_inicio and fecha_fin:\n",
    "        url_actual = construir_url_con_fechas(endpoint_url, fecha_inicio, fecha_fin)\n",
    "        print(f\"\\nüöÄ Comenzando extracci√≥n con filtro de fechas desde: {url_actual}\")\n",
    "    else:\n",
    "        url_actual = endpoint_url\n",
    "        print(f\"\\nüöÄ Comenzando extracci√≥n de todos los datos desde: {endpoint_url}\")\n",
    "    \n",
    "    pagina_actual = 1\n",
    "\n",
    "    while url_actual:\n",
    "        print(f\"üìÑ Obteniendo p√°gina {pagina_actual}...\")\n",
    "\n",
    "        try:\n",
    "            respuesta = requests.get(url_actual, headers=headers, timeout=10)\n",
    "            respuesta.raise_for_status()\n",
    "\n",
    "            respuesta_api = respuesta.json()\n",
    "            datos_pagina = respuesta_api.get('data', [])\n",
    "            pagination_info = respuesta_api.get('pagination', {})\n",
    "\n",
    "            todos_los_datos.extend(datos_pagina)\n",
    "\n",
    "            print(f\"‚úÖ P√°gina {pagina_actual}: {len(datos_pagina)} datos obtenidos\")\n",
    "            print(f\"üìä Total acumulado: {len(todos_los_datos)} datos\")\n",
    "            \n",
    "            total_pages = pagination_info.get('total_pages', 1)\n",
    "            print(f\"üìà P√°ginas restantes: {total_pages - pagina_actual}\")\n",
    "\n",
    "            url_actual = pagination_info.get('next')\n",
    "            pagina_actual += 1\n",
    "\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Error en la petici√≥n para la p√°gina {pagina_actual}: {e}\")\n",
    "            break\n",
    "\n",
    "    if aplicar_filtro_fechas:\n",
    "        print(f\"üéâ ¬°Extracci√≥n con filtro de fechas completada! Total: {len(todos_los_datos)} datos\")\n",
    "    else:\n",
    "        print(f\"üéâ ¬°Extracci√≥n completa terminada! Total: {len(todos_los_datos)} datos\")\n",
    "        \n",
    "    return todos_los_datos\n",
    "\n",
    "# Ejemplo de uso con el segmentador de fechas\n",
    "todos_los_datos_extraidos = {}\n",
    "\n",
    "for nombre_endpoint, url_endpoint in API_ENDPOINTS.items():\n",
    "    print(f\"\\nüîÑ --- Procesando endpoint: '{nombre_endpoint}' ---\")\n",
    "    \n",
    "    if FILTRAR_POR_FECHAS:\n",
    "        datos_obtenidos = obtener_datos_paginados(\n",
    "            url_endpoint, \n",
    "            aplicar_filtro_fechas=True, \n",
    "            fecha_inicio=FECHA_INICIO, \n",
    "            fecha_fin=FECHA_FIN\n",
    "        )\n",
    "    else:\n",
    "        datos_obtenidos = obtener_datos_paginados(url_endpoint)\n",
    "    \n",
    "    todos_los_datos_extraidos[nombre_endpoint] = datos_obtenidos\n",
    "    print(f\"--- Extracci√≥n de '{nombre_endpoint}' terminada. {len(datos_obtenidos)} registros obtenidos ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20b22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Conectando a MySQL...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"üöÄ Conectando a MySQL...\")\n",
    "    conexion = pymysql.connect(\n",
    "        host=DB_HOST,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD,\n",
    "        charset='utf8mb4'\n",
    "    )\n",
    "    cursor = conexion.cursor()\n",
    "    cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {DB_NAME}\")\n",
    "    cursor.execute(f\"USE {DB_NAME}\")\n",
    "    print(f\"‚úÖ Conectado a MySQL y usando la base: {DB_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al conectar a la base de datos: {e}\")\n",
    "    exit()\n",
    "\n",
    "def crear_e_insertar_tabla_actualiza(nombre_tabla: str, datos: list):\n",
    "    \"\"\"\n",
    "    Crea una tabla en la base de datos y la llena solo con registros nuevos o actualiza los existentes.\n",
    "    \"\"\"\n",
    "    if not datos:\n",
    "        print(f\"‚ö†Ô∏è No hay datos para la tabla '{nombre_tabla}'. Saliendo...\")\n",
    "        return\n",
    "\n",
    "    columnas = list(datos[0].keys())\n",
    "    columnas_con_tipos = [f\"{col} VARCHAR(255)\" for col in columnas]\n",
    "\n",
    "    # Asegurarse de que el 'id' sea la clave primaria\n",
    "    if 'id' in columnas_con_tipos:\n",
    "        id_index = columnas_con_tipos.index('id VARCHAR(255)')\n",
    "        columnas_con_tipos[id_index] = 'id INT PRIMARY KEY'\n",
    "\n",
    "    column_definitions = \", \".join(columnas_con_tipos)\n",
    "    create_table_sql = f\"CREATE TABLE IF NOT EXISTS {nombre_tabla} ({column_definitions})\"\n",
    "    cursor.execute(create_table_sql)\n",
    "\n",
    "    print(f\"üöÄ Insertando o actualizando registros en la tabla '{nombre_tabla}'...\")\n",
    "\n",
    "    # Construir la parte de actualizaci√≥n para ON DUPLICATE KEY UPDATE\n",
    "    update_clause = \", \".join([f\"{col}=VALUES({col})\" for col in columnas if col != 'id'])\n",
    "\n",
    "    sql_insert = f\"\"\"\n",
    "    INSERT INTO {nombre_tabla} ({', '.join(columnas)})\n",
    "    VALUES ({', '.join(['%s'] * len(columnas))})\n",
    "    ON DUPLICATE KEY UPDATE {update_clause}\n",
    "    \"\"\"\n",
    "\n",
    "    contador = 0\n",
    "    for item in datos:\n",
    "        try:\n",
    "            values = tuple(item.get(col) for col in columnas)\n",
    "            cursor.execute(sql_insert, values)\n",
    "            contador += cursor.rowcount  # Cuenta inserciones y actualizaciones\n",
    "        except Exception as error:\n",
    "            print(f\"‚ö†Ô∏è Error insertando/actualizando registro con id {item.get('id', 'N/A')}: {error}\")\n",
    "\n",
    "    conexion.commit()\n",
    "    print(f\"‚úÖ {contador} registros insertados o actualizados en '{nombre_tabla}'.\")\n",
    "\n",
    "# 1. Extraer los datos de todos los endpoints\n",
    "todos_los_datos_extraidos = {}\n",
    "for nombre_endpoint, url_endpoint in API_ENDPOINTS.items():\n",
    "    datos_obtenidos = obtener_datos_paginados(url_endpoint, aplicar_filtro_fechas=FILTRAR_POR_FECHAS, fecha_inicio=FECHA_INICIO, fecha_fin=FECHA_FIN)\n",
    "    todos_los_datos_extraidos[nombre_endpoint] = datos_obtenidos\n",
    "    print(f\"--- Extracci√≥n de '{nombre_endpoint}' terminada. ---\")\n",
    "\n",
    "# 2. Iterar sobre los datos extra√≠dos e insertarlos en la BD\n",
    "print(\"\\n--- Procesando e insertando/actualizando datos en MySQL ---\")\n",
    "for nombre_tabla, datos in todos_los_datos_extraidos.items():\n",
    "    crear_e_insertar_tabla_actualiza(nombre_tabla, datos)\n",
    "\n",
    "# 3. Mostrar estad√≠sticas por tabla\n",
    "print(\"\\n--- Consultando estad√≠sticas de las tablas ---\")\n",
    "for nombre_tabla in todos_los_datos_extraidos.keys():\n",
    "    try:\n",
    "        cursor.execute(f\"SELECT COUNT(*) as total FROM {nombre_tabla};\")\n",
    "        total_registros = cursor.fetchone()[0]\n",
    "        print(f\"üìä Total de registros en la tabla '{nombre_tabla}': {total_registros}\")\n",
    "\n",
    "        print(f\"üîç Mostrando los primeros 3 registros de '{nombre_tabla}':\")\n",
    "        cursor.execute(f\"SELECT * FROM {nombre_tabla} LIMIT 3;\")\n",
    "        column_names = [desc[0] for desc in cursor.description]\n",
    "        for row in cursor.fetchall():\n",
    "            print(dict(zip(column_names, row)))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al consultar la tabla '{nombre_tabla}': {e}\")\n",
    "\n",
    "\n",
    "with open(r\"C:\\Users\\bgacitua\\Desktop\\Repositorio_Compartido_GitHub\\logs_rflex\", 'a') as f:\n",
    "        f.write(f\"{datetime.now()}: Sincronizaci√≥n completada\\n\")\n",
    "\n",
    "# --- Cierre de conexi√≥n ---\n",
    "cursor.close()\n",
    "conexion.close()\n",
    "print(\"\\n‚úÖ Conexi√≥n cerrada correctamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entorno_premium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
